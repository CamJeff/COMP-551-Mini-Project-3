{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GOdzx6kUNIm"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the normalized dataset:"
      ],
      "metadata": {
        "id": "ufbeTBGtmUAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mean, std of the pixel intensity values\n",
        "def get_mean_and_std():\n",
        "  # Get unnormalized data with pixel values [0,1]\n",
        "  dataset = datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.ToTensor()\n",
        "  )\n",
        "\n",
        "  # Split into 60_000/1_000 = 60 batches for faster processing\n",
        "  loader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
        "  mean, std, num_batches = 0.0, 0.0, 0\n",
        "\n",
        "  for images, _ in loader:\n",
        "    batch_samples = images.size(0)\n",
        "\n",
        "    # Convert 28x28 arrray to 784x1\n",
        "    images = images.view(batch_samples, -1)\n",
        "\n",
        "    # Get mean and std for pixel values of the batch\n",
        "    mean += images.mean(dim=1).sum()\n",
        "    std += images.std(dim=1).sum()\n",
        "    num_batches += batch_samples\n",
        "\n",
        "  mean /= num_batches\n",
        "  std /= num_batches\n",
        "\n",
        "  return mean.item(), std.item()"
      ],
      "metadata": {
        "id": "VMThPQw1eUrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the train and test datasets loaders\n",
        "def get_datasets(is_cnn = False):\n",
        "  # Get the normalization mean and std\n",
        "  mean, std = get_mean_and_std()\n",
        "\n",
        "  # Convert and Normalize the data to a tensor (pixel values between 0 and 1) and flatten it to a 784 element list\n",
        "  ds_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean,), (std,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "  ])\n",
        "\n",
        "  # Same as before but keep the 28x28 structure if its a CNN\n",
        "  if is_cnn:\n",
        "    ds_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((mean,), (std,)),\n",
        "    ])\n",
        "\n",
        "  # Get the transformed train and test datset\n",
        "  complete_train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ds_transform)\n",
        "  complete_test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=ds_transform)\n",
        "\n",
        "  train_loader = DataLoader(complete_train_dataset, batch_size=100, shuffle=True)\n",
        "  test_loader = DataLoader(complete_test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "FoEQDKzFesgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model implementation"
      ],
      "metadata": {
        "id": "jFckfPSQetxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions and their derivatives\n",
        "class ReLU:\n",
        "  def activation(self,x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  def derivative(self,x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "  def activation(self,x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "  def derivative(self, x):\n",
        "    return 1 - np.square(self.activation(x))\n",
        "\n",
        "\n",
        "class LeakyReLU:\n",
        "  def __init__(self, y):\n",
        "    self.gamma = y\n",
        "\n",
        "  def activation(self, x):\n",
        "    return np.maximum(0,x) + self.gamma*np.minimum(0,x)\n",
        "\n",
        "  def derivative(self, x):\n",
        "    return np.where(x > 0, 1, self.gamma)\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def activation(self,x):\n",
        "    sum = np.sum(np.exp(x), axis=0, keepdims=True) # should work for a matrix now\n",
        "    return np.exp(x) / sum\n",
        "\n",
        "  def derivative(self,x): #assumes that x is already a softmax vector\n",
        "    n = x.shape[-1]\n",
        "    jacobian = np.zeros((n,n))\n",
        "    for i in range(0,n):\n",
        "      for j in range(0,n):\n",
        "        if i == j:\n",
        "          jacobian[i,j] = x[i] * (1-x[i])\n",
        "        else:\n",
        "          jacobian[i,j] = -x[i] * x[j]\n",
        "    return jacobian\n",
        "\n",
        "class MultiClassLoss:\n",
        "  def loss(self, pred, true): # assumes pred and true are vectors of the same size\n",
        "    return -np.sum(true * np.log(pred))\n",
        "\n",
        "  def derivative(self, pred, true): #returns the partial derivatives w.r.t. all the class probabilies\n",
        "    return -true / pred"
      ],
      "metadata": {
        "id": "3ti40GFlXjUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBackpropagation:\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    hidden_sizes = an int list that describes the number of units in each hidden layer\n",
        "    inner_activation = an object that will be the activation function for all hidden layers\n",
        "    final_activation = an object that will be the activation function for the output\n",
        "    loss = an object that represents the loss function\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_sizes, inner_activation, final_activation, loss, bias = True):\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.w_matrices = [None]\n",
        "        self.biases = [None]\n",
        "\n",
        "        for k in range(len(hidden_sizes)-1): #initialize V later when we recieved x\n",
        "          w = np.random.randn(hidden_sizes[k+1], hidden_sizes[k]) * .01   #intialize with random gaussian noise\n",
        "          self.w_matrices.append(w)\n",
        "\n",
        "          if (bias):\n",
        "            b = np.ones((1,hidden_sizes[k+1])) # keep separate for easier gradient calulation\n",
        "            self.biases.append(b)\n",
        "\n",
        "        # intialize the inner activation and the last activation (both are objects)\n",
        "        self.inner_fn = inner_activation\n",
        "        self.outer_fn = final_activation\n",
        "\n",
        "        #need to intialize what is the loss function\n",
        "        self.loss_fn = loss\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    given the features of 1 instance, compute the prediction\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "      self.hidden_units = []\n",
        "      self.activated_units = []\n",
        "\n",
        "      units = x.reshape(-1, 1)\n",
        "\n",
        "      #go through the hidden layers\n",
        "      for i in range(len(self.w_matrices) - 1):\n",
        "        # linear transformation on input\n",
        "        units = np.dot(self.w_matrices[i], units).T + self.biases[i]\n",
        "        units = units.reshape(-1, 1)\n",
        "        self.hidden_units.append(units.copy())\n",
        "\n",
        "        # activate\n",
        "        units = self.inner_fn.activation(units)\n",
        "        self.activated_units.append(units.copy())\n",
        "\n",
        "      #produce the prediction\n",
        "      y = np.dot(self.w_matrices[-1], units) + self.biases[-1].reshape(-1,1)\n",
        "      self.hidden_units.append(y.copy())\n",
        "\n",
        "      y = self.outer_fn.activation(y)\n",
        "      self.activated_units.append(y.copy())\n",
        "\n",
        "      return y\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    calculate the gradient of the weights using the given instance\n",
        "    \"\"\"\n",
        "    def backward(self, x, pred_y, true_y): # assumes we are given just 1 instance\n",
        "      # calculate the loss w.r.t. the output\n",
        "      dy_hat = self.loss_fn.derivative(pred_y,true_y)\n",
        "\n",
        "      # calculate the loss w.r.t. the outer activation\n",
        "      if (isinstance(self.outer_fn, Softmax)):\n",
        "        # simplify the formula so we dont need to calculate the jacobian\n",
        "        da_list = [(pred_y.reshape(-1, 1) - true_y.reshape(-1, 1)).T]\n",
        "      else:\n",
        "        da_list = [np.dot(dy_hat,self.outer_fn.derivative(self.hidden_units[-1])).T]\n",
        "\n",
        "      # check if the model has no hidden layers\n",
        "      if (len(self.w_matrices) == 1):\n",
        "        dw_list = [np.dot(x.reshape(-1,1), da_list[0]).T]\n",
        "        db_list = [da_list[0]]\n",
        "        return dw_list, db_list\n",
        "\n",
        "      # calculate for the outer layer -> y = fn(a) = f(Wc + b) = f(W * g(d) + b) = ....\n",
        "      dw_list = [np.dot(self.activated_units[-2], da_list[0]).T]\n",
        "      db_list = [da_list[0]]\n",
        "\n",
        "      # follow a similar formula for the hidden layers\n",
        "      tot = len(self.activated_units)\n",
        "      for i in range(1, tot - 1):\n",
        "        da = np.dot(da_list[-1], self.w_matrices[tot-i])\n",
        "        da = self.inner_fn.derivative(self.hidden_units[tot-i-1]).T * da\n",
        "        da_list.append(da)\n",
        "        dw_list.append(np.dot(self.activated_units[tot-i-2], da).T)\n",
        "        db_list.append(da)\n",
        "\n",
        "      #repeat one more time for the input layer\n",
        "      da = np.dot(da_list[-1], self.w_matrices[1])\n",
        "      da = self.inner_fn.derivative(self.hidden_units[0]).T * da\n",
        "      dw_list.append(np.dot(x.reshape(-1,1),da).T)\n",
        "      db_list.append(da)\n",
        "\n",
        "      return dw_list, db_list\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    fits the model using X and Y and evaluate the perfomance per epoch if given a test set\n",
        "    \"\"\"\n",
        "    def fit(self, X, Y, epoch, learning_rate = 0.05, testX = None, testY = None):\n",
        "      features = X.shape[-1]\n",
        "      classes = Y.shape[-1]\n",
        "\n",
        "      # initalize the input weight matrix and bias\n",
        "      if (self.hidden_sizes != []):\n",
        "        self.w_matrices[0] = np.random.randn(self.hidden_sizes[0], features) * .01\n",
        "        self.biases[0] = np.ones((1,self.hidden_sizes[0]))\n",
        "\n",
        "        # intialize the output weight matrix\n",
        "        self.w_matrices.append(np.random.randn(classes, self.hidden_sizes[-1]) * .01)\n",
        "        self.biases.append(np.ones((1,classes)))\n",
        "\n",
        "      else: # no hidden layers\n",
        "        self.w_matrices[0] = np.random.randn(classes, features) * .01\n",
        "        self.biases[0] = np.ones((1,classes))\n",
        "\n",
        "      # training setup\n",
        "\n",
        "      # determine how many weights to update\n",
        "      matrices = len(self.w_matrices)\n",
        "\n",
        "      # if given a test set, evaluate the performance of the model\n",
        "      evaluate = isinstance(testX, np.ndarray) and isinstance(testY, np.ndarray)\n",
        "\n",
        "      # initialize train_acc and test_acc (perfomance with no training)\n",
        "      if (evaluate):\n",
        "        train_result = self.predict(X)\n",
        "        test_result = self.predict(testX)\n",
        "        self.train_acc = [self.evaluate_acc(train_result, Y)]\n",
        "        self.test_acc = [self.evaluate_acc(test_result, testY)]\n",
        "\n",
        "      # determine how many instances we have\n",
        "      if (X.ndim == 1):\n",
        "        instances = 1\n",
        "      else:\n",
        "        instances = X.shape[0]\n",
        "\n",
        "      # SDG\n",
        "      for i in range(epoch):\n",
        "        for j in range(instances):\n",
        "\n",
        "          # get gradient for each instance\n",
        "          if (X.ndim == 1):\n",
        "            pred = self.forward(X).flatten()\n",
        "            grad_w, grad_b = self.backward(X, pred.reshape(-1,1), Y)\n",
        "          else:\n",
        "            pred = self.forward(X[j,:]).flatten()\n",
        "            grad_w, grad_b = self.backward(X[j,:], pred.reshape(-1,1), Y[j,:])\n",
        "\n",
        "          #update the weight backwards due to how grad_w and grad_b are stored\n",
        "          for k in range(matrices):\n",
        "            self.w_matrices[k] -= learning_rate * grad_w[matrices-1-k]\n",
        "            self.biases[k] -= learning_rate * grad_b[matrices-1-k]\n",
        "\n",
        "        # calculate performance per epoch\n",
        "        if (evaluate):\n",
        "          train_result = self.predict(X)\n",
        "          test_result = self.predict(testX)\n",
        "          self.train_acc.append(self.evaluate_acc(train_result, Y))\n",
        "          self.test_acc.append(self.evaluate_acc(test_result, testY))\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    given a set of predictions and true labels, determine the accuracy of the model\n",
        "    \"\"\"\n",
        "    def evaluate_acc(self, pred, true):\n",
        "      # assumes model is doing mulit-classification where classes = 1,2,...,k\n",
        "      # and that the true labels are one hot encoded\n",
        "      correct = 0\n",
        "      total = len(pred)\n",
        "      for i in range(total):\n",
        "        # determine which class is chosen\n",
        "        c = np.argmax(pred[i])\n",
        "        t = np.argmax(true[i])\n",
        "\n",
        "        # check true label\n",
        "        if c == t:\n",
        "          correct += 1\n",
        "      return correct / total\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    given a set of instances, compute the prediction\n",
        "    \"\"\"\n",
        "    def predict(self, X):\n",
        "      if (X.ndim == 1):\n",
        "        return self.forward(X)\n",
        "      else:\n",
        "        # need to do more than 1 forward pass\n",
        "        predictions = np.zeros((X.shape[0],10))\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "          p = self.forward(X[i,:]).flatten()\n",
        "          predictions[i,:] = self.forward(X[i,:]).flatten()\n",
        "\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "LCK0j5JjUkN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage:\n",
        "a = ReLU()\n",
        "b = Softmax()\n",
        "loss = MultiClassLoss()\n",
        "model = MLPBackpropagation([3,4], a, b, loss)\n",
        "\n",
        "# dataset\n",
        "c = np.array([[1,2], [3,2]])\n",
        "d = np.array([[0,1,0,0,0,0,0,0,0,0], [0,0,0,0,1,0,0,0,0,0]])\n",
        "u = np.array([[1,2], [3,2]])\n",
        "v = np.array([[0,1,0,0,0,0,0,0,0,0], [0,0,0,0,1,0,0,0,0,0]])\n",
        "\n",
        "model.fit(c,d,2,0.05,u,v)\n",
        "\n",
        "print(model.w_matrices)\n",
        "print(model.biases)\n",
        "\n",
        "# gives the performance of the model after each epoch (starts at epoch = 0)\n",
        "print(model.train_acc)\n",
        "print(model.test_acc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1kZiYXS-Z5Xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd0262b-424e-4fde-9d2b-12ce8b311a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[-0.00887012, -0.0006326 ],\n",
            "       [-0.00266154, -0.02137475],\n",
            "       [-0.00374831, -0.01258845]]), array([[-0.00148664, -0.00201177,  0.01248697],\n",
            "       [ 0.00076109, -0.00546214,  0.00258449],\n",
            "       [ 0.01771792,  0.00789625,  0.00988787],\n",
            "       [-0.02205463,  0.00132387, -0.00295447]]), array([[-0.01793693, -0.04234122, -0.01373082, -0.01655797],\n",
            "       [ 0.0565294 ,  0.08362823,  0.09009713,  0.06711527],\n",
            "       [-0.0240732 , -0.01543212, -0.0223693 , -0.01481796],\n",
            "       [-0.02161826, -0.01977443, -0.00334565,  0.00182939],\n",
            "       [ 0.08474325,  0.07628665,  0.08728935,  0.07434883],\n",
            "       [-0.02897312, -0.02374642, -0.0130974 , -0.00786239],\n",
            "       [-0.00971053, -0.01511436, -0.01293287, -0.02424816],\n",
            "       [-0.00645742, -0.03341212, -0.01950461, -0.0350789 ],\n",
            "       [-0.0144764 , -0.03204569,  0.00543394, -0.01485557],\n",
            "       [-0.02896831, -0.02589097, -0.01973847, -0.02130086]])]\n",
            "[array([[0.99998553, 0.9999694 , 1.00003303]]), array([[1.00199548, 1.0045735 , 1.00339156, 1.00195744]]), array([[0.98114595, 1.07540664, 0.98088325, 0.98019666, 1.0780526 ,\n",
            "        0.98082061, 0.98057744, 0.98121603, 0.9804463 , 0.98125452]])]\n",
            "[0.0, 0.5, 0.5]\n",
            "[0.0, 0.5, 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional preprocessing to the dataset: Encoding the categorical labels"
      ],
      "metadata": {
        "id": "btGDtEkDmeWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oneHotEncoding(Y):\n",
        "  # given an array of labels, one hot encode them\n",
        "  n = Y.shape[0]\n",
        "  encoded = np.zeros((n,10))\n",
        "\n",
        "  # one hot encoding (if class = k, set the k-th value = 1)\n",
        "  for i in range(n):\n",
        "    c = Y[i]\n",
        "    encoded[i,c] = 1\n",
        "\n",
        "  return encoded\n",
        "\n",
        "\n",
        "# get our normalized dataset\n",
        "training_data, testing_data = get_datasets()\n",
        "\n",
        "# split data to inputs and labels\n",
        "trainX, trainY = next(iter(training_data))\n",
        "testX, testY = next(iter(testing_data))\n",
        "\n",
        "# convert to numpy array\n",
        "trainX = trainX.numpy()\n",
        "trainY = trainY.numpy()\n",
        "testX = testX.numpy()\n",
        "testY = testY.numpy()\n",
        "\n",
        "# one hot encode the labels\n",
        "trainY = oneHotEncoding(trainY)\n",
        "testY = oneHotEncoding(testY)"
      ],
      "metadata": {
        "id": "bMa72zFhdlKS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}