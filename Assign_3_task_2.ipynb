{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9GOdzx6kUNIm"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions and their derivatives\n",
        "class ReLU:\n",
        "  def activation(self,x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  def derivative(self,x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "  def activation(self,x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "  def derivative(self, x):\n",
        "    return 1 - np.square(self.activation(x))\n",
        "\n",
        "\n",
        "class LeakyReLU:\n",
        "  def __init__(self, y):\n",
        "    self.gamma = y\n",
        "\n",
        "  def activation(self, x):\n",
        "    return np.maximum(0,x) + self.gamma*np.minimum(0,x)\n",
        "\n",
        "  def derivative(self, x):\n",
        "    return np.where(x > 0, 1, self.gamma)\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def activation(self,x):\n",
        "    sum = np.sum(np.exp(x), axis=1, keepdims=True) # should work for a matrix now\n",
        "    return np.exp(x) / sum\n",
        "\n",
        "  def derivative(self,x): #assumes that x is already a softmax vector\n",
        "    n = x.shape[-1]\n",
        "    jacobian = np.zeros((n,n))\n",
        "    for i in range(0,n):\n",
        "      for j in range(0,n):\n",
        "        if i == j:\n",
        "          jacobian[i,j] = x[i] * (1-x[i])\n",
        "        else:\n",
        "          jacobian[i,j] = -x[i] * x[j]\n",
        "    return jacobian\n",
        "\n",
        "class MultiClassLoss:\n",
        "  def loss(self, pred, true): # assumes pred and true are vectors of the same size\n",
        "    return -np.sum(true * np.log(pred))\n",
        "\n",
        "  def derivative(self, pred, true): #returns the partial derivatives w.r.t. all the class probabilies\n",
        "    return -true / pred"
      ],
      "metadata": {
        "id": "3ti40GFlXjUe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBackpropagation:\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    hidden_sizes = an int list that describes the number of units in each hidden layer\n",
        "    inner_activation = an object that will be the activation function for all hidden layers\n",
        "    final_activation = an object that will be the activation function for the output\n",
        "    loss = an object that represents the loss function\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_sizes, inner_activation, final_activation, loss, bias = True):\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.w_matrices = [None]\n",
        "        self.biases = [None]\n",
        "\n",
        "        for k in range(len(hidden_sizes)-1): #initialize V later when we recieved x\n",
        "          w = np.random.randn(hidden_sizes[k+1], hidden_sizes[k]) * .01   #intialize with random gaussian noise\n",
        "          self.w_matrices.append(w)\n",
        "\n",
        "          if (bias):\n",
        "            b = np.ones((1,hidden_sizes[k+1])) # keep separate for easier gradient calulation\n",
        "            self.biases.append(b)\n",
        "\n",
        "        # intialize the inner activation and the last activation (both are objects)\n",
        "        self.inner_fn = inner_activation\n",
        "        self.outer_fn = final_activation\n",
        "\n",
        "        #need to intialize what is the loss function\n",
        "        self.loss_fn = loss\n",
        "\n",
        "    def forward(self, x):\n",
        "      self.hidden_units = []\n",
        "      self.activated_units = []\n",
        "\n",
        "      units = x.reshape(-1, 1)\n",
        "\n",
        "      #go through the hidden layers\n",
        "      for i in range(len(self.w_matrices) - 1):\n",
        "        # linear transformation on input\n",
        "        units = np.dot(self.w_matrices[i], units).T + self.biases[i]\n",
        "        units = units.reshape(-1, 1)\n",
        "        self.hidden_units.append(units.copy())\n",
        "\n",
        "        # activate\n",
        "        units = self.inner_fn.activation(units)\n",
        "        self.activated_units.append(units.copy())\n",
        "\n",
        "      #produce the prediction\n",
        "      y = np.dot(self.w_matrices[-1], units) + self.biases[-1].reshape(-1,1)\n",
        "      self.hidden_units.append(y.copy())\n",
        "\n",
        "      y = self.outer_fn.activation(y)\n",
        "      self.activated_units.append(y.copy())\n",
        "\n",
        "      return y\n",
        "\n",
        "\n",
        "    def backward(self, x, pred_y, true_y): # assumes we are given just 1 instance\n",
        "      # calculate the loss w.r.t. the output\n",
        "      dy_hat = self.loss_fn.derivative(pred_y,true_y)\n",
        "\n",
        "      # calculate the loss w.r.t. the outer activation\n",
        "      if (isinstance(self.outer_fn, Softmax)):\n",
        "        # simplify the formula so we dont need to calculate the jacobian\n",
        "        da_list = [(pred_y.reshape(-1, 1) - true_y.reshape(-1, 1)).T]\n",
        "      else:\n",
        "        da_list = [np.dot(dy_hat,self.outer_fn.derivative(self.activated_units[-1])).T]\n",
        "\n",
        "      # check if the model has no hidden layers\n",
        "      if (len(self.w_matrices) == 1):\n",
        "        dw_list = [np.dot(x.reshape(-1,1), da_list[0]).T]\n",
        "        db_list = [da_list[0]]\n",
        "        return dw_list, db_list\n",
        "\n",
        "      # calculate for the outer layer -> y = fn(a) = f(Wc + b) = f(W * g(d) + b) = ....\n",
        "      dw_list = [np.dot(self.activated_units[-2], da_list[0]).T]\n",
        "      db_list = [da_list[0]]\n",
        "\n",
        "      # follow a similar formula for the hidden layers\n",
        "      tot = len(self.activated_units)\n",
        "      for i in range(1, tot - 1):\n",
        "        da = np.dot(da_list[-1], self.w_matrices[tot-i])\n",
        "        da = self.inner_fn.derivative(self.activated_units[tot-i-1]).T * da\n",
        "        da_list.append(da)\n",
        "        dw_list.append(np.dot(self.activated_units[tot-i-2], da).T)\n",
        "        db_list.append(da)\n",
        "\n",
        "      #repeat one more time for the input layer\n",
        "      da = np.dot(da_list[-1], self.w_matrices[1])\n",
        "      da = self.inner_fn.derivative(self.activated_units[0]).T * da\n",
        "      dw_list.append(np.dot(x.reshape(-1,1),da).T)\n",
        "      db_list.append(da)\n",
        "\n",
        "      return dw_list, db_list\n",
        "\n",
        "    def fit(self, X, Y, epoch, learning_rate = 0.05, testX = None, testY = None):\n",
        "      features = X.shape[-1]\n",
        "      classes = Y.shape[-1]\n",
        "\n",
        "      # initalize the input weight matrix and bias\n",
        "      if (self.hidden_sizes != []):\n",
        "        self.w_matrices[0] = np.random.randn(self.hidden_sizes[0], features) * .01\n",
        "        self.biases[0] = np.ones((1,self.hidden_sizes[0]))\n",
        "\n",
        "        # intialize the output weight matrix\n",
        "        self.w_matrices.append(np.random.randn(classes, self.hidden_sizes[-1]) * .01)\n",
        "        self.biases.append(np.ones((1,classes)))\n",
        "\n",
        "      else: # no hidden layers\n",
        "        self.w_matrices[0] = np.random.randn(classes, features) * .01\n",
        "        self.biases[0] = np.ones((1,classes))\n",
        "\n",
        "\n",
        "      # tranning setup\n",
        "      self.train_acc = []\n",
        "      self.test_acc = []\n",
        "      matrices = len(self.w_matrices)\n",
        "      evaluate = testX != None and testY != None\n",
        "\n",
        "      if (X.ndim == 1):\n",
        "        instances = 1\n",
        "      else:\n",
        "        instances = X.shape[0]\n",
        "\n",
        "      # SDG\n",
        "      for i in range(epoch):\n",
        "        for j in range(instances):\n",
        "\n",
        "          # get gradient for each instance\n",
        "          if (X.ndim == 1):\n",
        "            pred = self.forward(X).flatten()\n",
        "            grad_w, grad_b = self.backward(X, pred.reshape(-1,1), Y)\n",
        "          else:\n",
        "            pred = self.forward(X[j,:]).flatten()\n",
        "            grad_w, grad_b = self.backward(X[j,:], pred.reshape(-1,1), Y[j,:])\n",
        "\n",
        "          #update the weight backwards due to how grad_w and grad_b are stored\n",
        "          for k in range(matrices):\n",
        "            self.w_matrices[k] -= learning_rate * grad_w[matrices-1-k]\n",
        "            self.biases[k] -= learning_rate * grad_b[matrices-1-k]\n",
        "\n",
        "        # calculate performance per epoch\n",
        "        if (evaluate):\n",
        "          train_result = self.predict(X)\n",
        "          test_result = self.predict(testX)\n",
        "          self.train_acc.append(self.evaluate_acc(train_result, Y))\n",
        "          self.test_acc.append(self.evaluate_acc(test_result, testY))\n",
        "\n",
        "    def evaluate_acc(self, pred, true):\n",
        "      # assumes model is doing mulit-classification where classes = 1,2,...,k\n",
        "      correct = 0\n",
        "      total = len(pred)\n",
        "      for i in range(total):\n",
        "        # determine which class is chosen\n",
        "        c = np.argmax(pred[i])\n",
        "        t = np.argmax(true[i])\n",
        "\n",
        "        # check true label\n",
        "        if c == t:\n",
        "          correct += 1\n",
        "      return correct / total\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      if (X.ndim == 1):\n",
        "        return self.forward(X)\n",
        "      else:\n",
        "        # need to do more than 1 forward pass\n",
        "        predictions = np.zeros(X.shape)\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "          p = self.forward(X[i,:]).flatten()\n",
        "          print(p.shape)\n",
        "          print(predictions[i,:].shape)\n",
        "          predictions[i,:] = self.forward(X[i,:]).flatten()\n",
        "\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "LCK0j5JjUkN6"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage:\n",
        "fn = ReLU()\n",
        "g = Softmax()\n",
        "loss = MultiClassLoss()\n",
        "model = MLPBackpropagation([3,4], fn, g, loss)\n",
        "\n",
        "x = np.array([[1,2], [3,2]])\n",
        "y = np.array([[0,1], [0,1]])\n",
        "model.fit(x,y,4)\n",
        "\n",
        "print(model.w_matrices)\n",
        "print(model.biases)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1kZiYXS-Z5Xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b301214-314d-4d4f-f917-33a1bdc53b99"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.02737603, 0.01838725],\n",
            "       [0.0119414 , 0.03444603],\n",
            "       [0.01934491, 0.01441589]]), array([[0.07962928, 0.07754506, 0.08036094],\n",
            "       [0.0601658 , 0.05521002, 0.08625459],\n",
            "       [0.08424674, 0.07830913, 0.07799915],\n",
            "       [0.07557669, 0.08331489, 0.08377116]]), array([[-0.43307044, -0.418664  , -0.43611015, -0.44512337],\n",
            "       [-0.01564098, -0.00394903,  0.00196755,  0.00162914]])]\n",
            "[array([[1.00879205, 1.00848064, 1.01124931]]), array([[1.0718843 , 1.06913304, 1.07288489, 1.07653005]]), array([[0.6, 1. ]])]\n"
          ]
        }
      ]
    }
  ]
}