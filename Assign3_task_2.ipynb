{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9GOdzx6kUNIm"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions and their derivatives\n",
        "class ReLU:\n",
        "  def activation(self,x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "  def derivative(self,x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "  def activation(self,x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "  def derivative(self, x):\n",
        "    return 1 - np.square(self.activation(x))\n",
        "\n",
        "\n",
        "class LeakyReLU:\n",
        "  def __init__(self, y):\n",
        "    self.gamma = y\n",
        "\n",
        "  def activation(self, x):\n",
        "    return np.maximum(0,x) + self.gamma*np.minimum(0,x)\n",
        "\n",
        "  def derivative(self, x):\n",
        "    return np.where(x > 0, 1, self.gamma)\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def activation(self,x):\n",
        "    sum = np.sum(np.exp(x), axis=1, keepdims=True) # should work for a matrix now\n",
        "    return np.exp(x) / sum\n",
        "\n",
        "  def derivative(self,x): #assumes that x is already a softmax vector\n",
        "    n = x.shape[-1]\n",
        "    jacobian = np.zeros((n,n))\n",
        "    for i in range(0,n):\n",
        "      for j in range(0,n):\n",
        "        if i == j:\n",
        "          jacobian[i,j] = x[i] * (1-x[i])\n",
        "        else:\n",
        "          jacobian[i,j] = -x[i] * x[j]\n",
        "    return jacobian\n",
        "\n",
        "class MultiClassLoss:\n",
        "  def loss(self, pred, true): # assumes pred and true are vectors of the same size\n",
        "    return -np.sum(true * np.log(pred))\n",
        "\n",
        "  def derivative(self, pred, true): #returns the partial derivatives w.r.t. all the class probabilies\n",
        "    return -true / pred"
      ],
      "metadata": {
        "id": "3ti40GFlXjUe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPBackpropagation:\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    hidden_sizes = an int list that describes the number of units in each hidden layer\n",
        "    inner_activation = an object that will be the activation function for all hidden layers\n",
        "    final_activation = an object that will be the activation function for the output\n",
        "    loss = an object that represents the loss function\n",
        "    Note: If hidden_sizes = [], there is no hidden layer\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_sizes, inner_activation, final_activation, loss, bias = True):\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.w_matrices = [None]\n",
        "        self.biases = [None]\n",
        "\n",
        "        for k in range(len(hidden_sizes)-1): #initialize V later when we recieved x\n",
        "          w = np.random.randn(hidden_sizes[k+1], hidden_sizes[k]) * .01   #intialize with random gaussian noise\n",
        "          self.w_matrices.append(w)\n",
        "\n",
        "          if (bias):\n",
        "            b = np.ones((1,hidden_sizes[k+1])) # keep separate for easier gradient calulation\n",
        "            self.biases.append(b)\n",
        "\n",
        "        # intialize the inner activation and the last activation (both are objects)\n",
        "        self.inner_fn = inner_activation\n",
        "        self.outer_fn = final_activation\n",
        "\n",
        "        #need to intialize what is the loss function\n",
        "        self.loss_fn = loss\n",
        "\n",
        "    def forward(self, x):\n",
        "      self.hidden_units = []\n",
        "      self.activated_units = []\n",
        "\n",
        "      units = x.reshape(-1, 1)\n",
        "\n",
        "      #go through the hidden layers\n",
        "      for i in range(len(self.w_matrices) - 1):\n",
        "        # linear transformation on input\n",
        "        units = np.dot(self.w_matrices[i], units).T + self.biases[i]\n",
        "        units = units.reshape(-1, 1)\n",
        "        self.hidden_units.append(units.copy())\n",
        "\n",
        "        # activate\n",
        "        units = self.inner_fn.activation(units)\n",
        "        self.activated_units.append(units.copy())\n",
        "\n",
        "      #produce the prediction\n",
        "      y = np.dot(self.w_matrices[-1], units) + self.biases[-1].reshape(-1,1)\n",
        "      self.hidden_units.append(y.copy())\n",
        "\n",
        "      y = self.outer_fn.activation(y)\n",
        "      self.activated_units.append(y.copy())\n",
        "\n",
        "      return y\n",
        "\n",
        "\n",
        "    def backward(self, x, pred_y, true_y): # assumes we are given just 1 instance\n",
        "      # calculate the loss w.r.t. the output\n",
        "      dy_hat = self.loss_fn.derivative(pred_y,true_y)\n",
        "\n",
        "      # calculate the loss w.r.t. the outer activation\n",
        "      if (isinstance(self.outer_fn, Softmax)):\n",
        "        # simplify the formula so we dont need to calculate the jacobian\n",
        "        da_list = [(pred_y.reshape(-1, 1) - true_y.reshape(-1, 1)).T]\n",
        "      else:\n",
        "        da_list = [np.dot(dy_hat,self.outer_fn.derivative(self.hidden_units[-1])).T]\n",
        "\n",
        "      # check if the model has no hidden layers\n",
        "      if (len(self.w_matrices) == 1):\n",
        "        dw_list = [np.dot(x.reshape(-1,1), da_list[0]).T]\n",
        "        db_list = [da_list[0]]\n",
        "        return dw_list, db_list\n",
        "\n",
        "      # calculate for the outer layer -> y = fn(a) = f(Wc + b) = f(W * g(d) + b) = ....\n",
        "      dw_list = [np.dot(self.activated_units[-2], da_list[0]).T]\n",
        "      db_list = [da_list[0]]\n",
        "\n",
        "      # follow a similar formula for the hidden layers\n",
        "      tot = len(self.activated_units)\n",
        "      for i in range(1, tot - 1):\n",
        "        da = np.dot(da_list[-1], self.w_matrices[tot-i])\n",
        "        da = self.inner_fn.derivative(self.hidden_units[tot-i-1]).T * da\n",
        "        da_list.append(da)\n",
        "        dw_list.append(np.dot(self.activated_units[tot-i-2], da).T)\n",
        "        db_list.append(da)\n",
        "\n",
        "      #repeat one more time for the input layer\n",
        "      da = np.dot(da_list[-1], self.w_matrices[1])\n",
        "      da = self.inner_fn.derivative(self.hidden_units[0]).T * da\n",
        "      dw_list.append(np.dot(x.reshape(-1,1),da).T)\n",
        "      db_list.append(da)\n",
        "\n",
        "      return dw_list, db_list\n",
        "\n",
        "    def fit(self, X, Y, epoch, learning_rate = 0.05, testX = None, testY = None):\n",
        "      features = X.shape[-1]\n",
        "      classes = Y.shape[-1]\n",
        "\n",
        "      # initalize the input weight matrix and bias\n",
        "      if (self.hidden_sizes != []):\n",
        "        self.w_matrices[0] = np.random.randn(self.hidden_sizes[0], features) * .01\n",
        "        self.biases[0] = np.ones((1,self.hidden_sizes[0]))\n",
        "\n",
        "        # intialize the output weight matrix\n",
        "        self.w_matrices.append(np.random.randn(classes, self.hidden_sizes[-1]) * .01)\n",
        "        self.biases.append(np.ones((1,classes)))\n",
        "\n",
        "      else: # no hidden layers\n",
        "        self.w_matrices[0] = np.random.randn(classes, features) * .01\n",
        "        self.biases[0] = np.ones((1,classes))\n",
        "\n",
        "\n",
        "      # training setup\n",
        "      self.train_acc = []\n",
        "      self.test_acc = []\n",
        "      matrices = len(self.w_matrices)\n",
        "      evaluate = testX != None and testY != None\n",
        "\n",
        "      if (X.ndim == 1):\n",
        "        instances = 1\n",
        "      else:\n",
        "        instances = X.shape[0]\n",
        "\n",
        "      # SDG\n",
        "      for i in range(epoch):\n",
        "        for j in range(instances):\n",
        "\n",
        "          # get gradient for each instance\n",
        "          if (X.ndim == 1):\n",
        "            pred = self.forward(X).flatten()\n",
        "            grad_w, grad_b = self.backward(X, pred.reshape(-1,1), Y)\n",
        "          else:\n",
        "            pred = self.forward(X[j,:]).flatten()\n",
        "            grad_w, grad_b = self.backward(X[j,:], pred.reshape(-1,1), Y[j,:])\n",
        "\n",
        "          #update the weight backwards due to how grad_w and grad_b are stored\n",
        "          for k in range(matrices):\n",
        "            self.w_matrices[k] -= learning_rate * grad_w[matrices-1-k]\n",
        "            self.biases[k] -= learning_rate * grad_b[matrices-1-k]\n",
        "\n",
        "        # calculate performance per epoch\n",
        "        if (evaluate):\n",
        "          train_result = self.predict(X)\n",
        "          test_result = self.predict(testX)\n",
        "          self.train_acc.append(self.evaluate_acc(train_result, Y))\n",
        "          self.test_acc.append(self.evaluate_acc(test_result, testY))\n",
        "\n",
        "    def evaluate_acc(self, pred, true):\n",
        "      # assumes model is doing mulit-classification where classes = 1,2,...,k\n",
        "      correct = 0\n",
        "      total = len(pred)\n",
        "      for i in range(total):\n",
        "        # determine which class is chosen\n",
        "        c = np.argmax(pred[i])\n",
        "        t = np.argmax(true[i])\n",
        "\n",
        "        # check true label\n",
        "        if c == t:\n",
        "          correct += 1\n",
        "      return correct / total\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      if (X.ndim == 1):\n",
        "        return self.forward(X)\n",
        "      else:\n",
        "        # need to do more than 1 forward pass\n",
        "        predictions = np.zeros(X.shape)\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "          p = self.forward(X[i,:]).flatten()\n",
        "          print(p.shape)\n",
        "          print(predictions[i,:].shape)\n",
        "          predictions[i,:] = self.forward(X[i,:]).flatten()\n",
        "\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "LCK0j5JjUkN6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage:\n",
        "fn = ReLU()\n",
        "g = Softmax()\n",
        "loss = MultiClassLoss()\n",
        "model = MLPBackpropagation([3,4], fn, g, loss)\n",
        "\n",
        "x = np.array([[1,2], [3,2]])\n",
        "y = np.array([[0,1], [0,1]])\n",
        "model.fit(x,y,7)\n",
        "\n",
        "print(model.w_matrices)\n",
        "print(model.biases)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1kZiYXS-Z5Xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584305c0-a982-49b6-918c-68981bfb18b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.32932072, 0.29620182],\n",
            "       [0.31980626, 0.30286481],\n",
            "       [0.28678625, 0.25913755]]), array([[0.3419859 , 0.33553531, 0.33033602],\n",
            "       [0.36720648, 0.36316423, 0.34690762],\n",
            "       [0.37902833, 0.37389422, 0.33066908],\n",
            "       [0.36509154, 0.36612979, 0.32101501]]), array([[-0.95647646, -1.00861114, -1.00263221, -0.9857562 ],\n",
            "       [-0.02452771,  0.01220209, -0.01289178,  0.00363596]])]\n",
            "[array([[1.14552794, 1.14145241, 1.12775439]]), array([[1.24588091, 1.26814382, 1.2601539 , 1.25605513]]), array([[0.3, 1. ]])]\n"
          ]
        }
      ]
    }
  ]
}